{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4c6d85-bed5-4640-a0b1-1873beec17dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Desktop\\enroll_embeddings\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "torchvision is not available - cannot save figures\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torchaudio\n",
    "from scipy.spatial.distance import cosine\n",
    "from speechbrain.pretrained import EncoderClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12705c19-db8c-4c69-9218-f691d74b2502",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "JSON_FILE = \"enrolled_users.json\"  # store embeddings here\n",
    "MIN_DURATION = 10  # minimum recording time in seconds\n",
    "SILENCE_THRESHOLD = 0.1  # RMS threshold to detect silence\n",
    "SILENCE_DURATION = 2  # seconds of silence to auto-stop\n",
    "\n",
    "# Load model once\n",
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\")\n",
    "\n",
    "# -----------------------------\n",
    "# Functions\n",
    "# -----------------------------\n",
    "\n",
    "def record_audio(min_duration=MIN_DURATION):\n",
    "    \"\"\"Record audio for at least min_duration and stop when user is silent.\"\"\"\n",
    "    print(f\"ðŸŽ™ Speak now (at least {min_duration} seconds). Recording will stop when you're silent...\")\n",
    "\n",
    "    samplerate = 16000\n",
    "    channels = 1\n",
    "\n",
    "    recording = []\n",
    "    silence_time = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    stream = sd.InputStream(samplerate=samplerate, channels=channels, dtype='float32')\n",
    "    stream.start()\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            data, _ = stream.read(int(samplerate * 0.2))  # 0.2 sec chunks\n",
    "            chunk = np.copy(data[:, 0])\n",
    "            recording.append(chunk)\n",
    "\n",
    "            # RMS to detect silence\n",
    "            rms = np.sqrt(np.mean(chunk ** 2))\n",
    "            if rms < SILENCE_THRESHOLD:\n",
    "                silence_time += 0.2\n",
    "            else:\n",
    "                silence_time = 0\n",
    "\n",
    "            duration = time.time() - start_time\n",
    "            if duration >= min_duration and silence_time >= SILENCE_DURATION:\n",
    "                break\n",
    "    finally:\n",
    "        stream.stop()\n",
    "        stream.close()\n",
    "\n",
    "    audio = np.concatenate(recording, axis=0)\n",
    "    print(f\"âœ… Recorded {len(audio)/samplerate:.2f} seconds of audio.\")\n",
    "    return audio, samplerate\n",
    "\n",
    "def extract_embedding(audio, samplerate):\n",
    "    \"\"\"Extract speaker embedding from audio array.\"\"\"\n",
    "    temp_path = \"temp_audio.wav\"\n",
    "    sf.write(temp_path, audio, samplerate)\n",
    "\n",
    "    signal, fs = torchaudio.load(temp_path)  # [channels, time]\n",
    "\n",
    "    # Convert to mono if stereo\n",
    "    if signal.shape[0] > 1:\n",
    "        signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "\n",
    "    # Resample if needed\n",
    "    if fs != samplerate:\n",
    "        transform = torchaudio.transforms.Resample(orig_freq=fs, new_freq=samplerate)\n",
    "        signal = transform(signal)\n",
    "\n",
    "    # Now make it [batch, time]\n",
    "    signal = signal.squeeze(0)            # [time]\n",
    "    signal = signal.unsqueeze(0)          # [1, time] = batch size 1\n",
    "\n",
    "    # Pass directly to encode_batch\n",
    "    embedding_tensor = classifier.encode_batch(signal)\n",
    "    embedding = embedding_tensor.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    os.remove(temp_path)\n",
    "    return embedding.tolist()\n",
    "\n",
    "def load_enrolled_data():\n",
    "    if os.path.exists(JSON_FILE):\n",
    "        with open(JSON_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_enrolled_data(data):\n",
    "    with open(JSON_FILE, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "def enroll_user(name):\n",
    "    \"\"\"Enroll a user with the given name\"\"\"\n",
    "    try:\n",
    "        audio, sr = record_audio(min_duration=MIN_DURATION)\n",
    "        embedding = extract_embedding(audio, sr)\n",
    "        enrolled_data = load_enrolled_data()\n",
    "        \n",
    "        # If name already exists, append to existing embeddings\n",
    "        if name in enrolled_data:\n",
    "            enrolled_data[name].append(embedding)\n",
    "        else:\n",
    "            enrolled_data[name] = [embedding]\n",
    "            \n",
    "        save_enrolled_data(enrolled_data)\n",
    "        return True, f\"User '{name}' enrolled successfully!\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Error during enrollment: {str(e)}\"\n",
    "\n",
    "def check_user(threshold=0.3):\n",
    "    \"\"\"Check if a user is already enrolled\"\"\"\n",
    "    try:\n",
    "        enrolled_data = load_enrolled_data()\n",
    "        if not enrolled_data:\n",
    "            return False, \"No users enrolled yet.\", None, None\n",
    "\n",
    "        audio, sr = record_audio(min_duration=MIN_DURATION)\n",
    "        test_emb = np.array(extract_embedding(audio, sr)).flatten()\n",
    "\n",
    "        best_name = None\n",
    "        best_score = float('inf')\n",
    "\n",
    "        for name, emb_list in enrolled_data.items():\n",
    "            for emb in emb_list:\n",
    "                emb_vec = np.array(emb).flatten()\n",
    "                score = cosine(test_emb, emb_vec)\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_name = name\n",
    "\n",
    "        if best_score < threshold:\n",
    "            return True, f\"Match: {best_name} (distance={best_score:.3f})\", best_name, best_score\n",
    "        else:\n",
    "            return False, f\"No match under threshold. Closest guess: {best_name} (distance={best_score:.3f})\", best_name, best_score\n",
    "    except Exception as e:\n",
    "        return False, f\"Error during verification: {str(e)}\", None, None\n",
    "\n",
    "def get_enrolled_users():\n",
    "    \"\"\"Get list of all enrolled users\"\"\"\n",
    "    enrolled_data = load_enrolled_data()\n",
    "    return list(enrolled_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "660718a5-b792-4075-8896-8f96621ff02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asttokens==3.0.0\n",
      "blinker==1.9.0\n",
      "certifi==2025.8.3\n",
      "cffi==2.0.0\n",
      "charset-normalizer==3.4.3\n",
      "click==8.3.0\n",
      "colorama==0.4.6\n",
      "comm==0.2.3\n",
      "debugpy==1.8.17\n",
      "decorator==5.2.1\n",
      "exceptiongroup==1.3.0\n",
      "executing==2.2.1\n",
      "filelock==3.19.1\n",
      "Flask==2.3.3\n",
      "fsspec==2025.9.0\n",
      "huggingface-hub==0.35.0\n",
      "HyperPyYAML==1.2.2\n",
      "idna==3.10\n",
      "ipykernel==6.30.1\n",
      "ipython==8.37.0\n",
      "itsdangerous==2.2.0\n",
      "jedi==0.19.2\n",
      "Jinja2==3.1.6\n",
      "joblib==1.5.2\n",
      "jupyter_client==8.6.3\n",
      "jupyter_core==5.8.1\n",
      "MarkupSafe==3.0.2\n",
      "matplotlib-inline==0.1.7\n",
      "mpmath==1.3.0\n",
      "nest-asyncio==1.6.0\n",
      "networkx==3.4.2\n",
      "numpy==1.24.3\n",
      "packaging==25.0\n",
      "parso==0.8.5\n",
      "platformdirs==4.4.0\n",
      "prompt_toolkit==3.0.52\n",
      "psutil==7.1.0\n",
      "pure_eval==0.2.3\n",
      "pycparser==2.23\n",
      "Pygments==2.19.2\n",
      "python-dateutil==2.9.0.post0\n",
      "pywin32==311\n",
      "PyYAML==6.0.2\n",
      "pyzmq==27.1.0\n",
      "requests==2.32.5\n",
      "ruamel.yaml==0.18.15\n",
      "ruamel.yaml.clib==0.2.12\n",
      "scipy==1.11.3\n",
      "sentencepiece==0.2.1\n",
      "six==1.17.0\n",
      "sounddevice==0.4.6\n",
      "soundfile==0.12.1\n",
      "speechbrain==0.5.15\n",
      "stack-data==0.6.3\n",
      "sympy==1.14.0\n",
      "torch==2.0.1\n",
      "torchaudio==2.0.2\n",
      "tornado==6.5.2\n",
      "tqdm==4.67.1\n",
      "traitlets==5.14.3\n",
      "typing_extensions==4.15.0\n",
      "urllib3==2.5.0\n",
      "wcwidth==0.2.13\n",
      "Werkzeug==3.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23bb5549-4d7e-4072-a132-b15600de6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8314dc-e840-4dba-a2a6-614207c389c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (enroll_embedding)",
   "language": "python",
   "name": "enroll_embedding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
